AI: DO NOT EDIT

we do not use pijul we borrow the graph conveniences and the methodology only.

we use the pijul reversible graph in libpijul instead of regex in our transpilation.  we can fix a smaller gap of graph specific transforms with a solid reversible graph semantic foundation.

While we consider IR a transpilation long term target, we only honor ORBIT SCANNER induction to quantify alternating { tuple< anchor >, evidenceSpan } in order to enforce a highly tiled and locality-oriented diffusion model of parsing from the terminal (meaning "all the way to the end") typeviddence and the anchors which fall out, creating region and block terraced topologies of emission and parent graph data about the artifacts.

To this end the wide speculative scanner is intended to create a numerical plasma sifter to align with the cadence and delimiting  of tokens assigned or assigning anchors.

We explicitly limit the IO to the orbit scanner, first creating the terminal typevidence span capable of providing hints about bounded IO input.  This allows for large speculative lojngest-match behaviors with highly local non-pointer counters until the moment of commitment.

This project has gone on so long, under so much training bias  under claude code, withotu meaningfull progress do to persistent avoidance of novel concepts in order to write demo AST code which doesnt mach the reactor based IO channelization and discrimination from e.g. unammed pipes with ambiguous protocols  and grammers, simultaneously, allowing for packets of network to be the input for the plasma

Reversibility in the transpiler is seen as a means to minimize  the gap between isomorphic source constructs and transpiled destination syntax asnd to gain an amount of reflexive symetry as a graph, expli8citly a graph of constructions, while also runtime graph of graph nodes based on the compiled-in scanner orbits.

Some semantic gaps like C macros and C++ templates define language specific conversions, we hope to unify these potentially able to be a real cfront on demand via the scanner, and on par with cppfront, and with llvm c-outputs

MLIR is the middleware conveyance of emitters at this time,  hosting a platform for which front-ir can assume AOT and jit  level c++ move perfection against a reengineered allocationstrategy at compile time to rehome, rescope, and perform, memory safety hueristics and code optimizations prior to final MLIR scaffolding, thus the node structure that was checked in using Sea Of Nodes specifically, having used gh repo ClifClick/Simple compiler  as a jumpstart originally training the llm chapter by chapter.

The strategy is once again, to enable the terminal evidencespan at least to the degree of relevant scope, to supply significant closure of confix recognition



we should be fishing the relevant code out of git, especially the c++ pijul routines

We have a 3-way isomorphic n-way transpiler c,c++,cpp2 with a initial simpler 2-way json<->yaml you must build into the scanner as a dogfooding serde lib for our semantic graph, which is the basis for the graph-based transpilation.

based on this architecture, ona  scale from 0 regex, to 11, LLM we want to arrive at the widest most tiled most locality of reference implementatin of using dominant bits as discriminators and eliminators to bake the plasma into  regions like a terraced rice patty.  
then we switch to the micr and create boundary zones for instance, 1 bit to memoize all the classes that are elimiinated from an input span, in order to extend the reach of cache line retention at conjecture time.  we may arrive at the liuxury of ingesting long range 1 bit fields, closer fields of 2 bits, and 3 bits im told can discriminate ascii efficiently, allowing for meta evidence ranges beyond a dense evidence span

imports should be pre-macro-processed.

Templates and macro relationships should be mapped, catch as catch can ; we want to be able to map the macro and template relationships in a way that allows us to preserve the semantics of the original code while also enabling us to generate efficient and correct C++ code or semantic C from templates and metafunctions , a very rare option.  pragmatic observatinos are as good as a language lawyer until; proven false, and we can use the graph to preserve the relationships between macros and templates, allowing us to generate code that is both correct and efficient and that preserves the semantics of the original code.  This is a key part of our approach to transpilation, and it allows us to handle complex C++ code that makes heavy use of templates and macros.

CPP2 dialect has some unique customizations in this codebase adding a cpp2 markdown block interpreted as a comment, creating a transpiled to NOOP c++ module named <CAS>.cppm or an empty C macro.  

CAS (Content Address Storage) of the markdown is defined as adler64 hashes on all block's lines catted together, and the hash is used as a unique identifier for the block.  This allows us to preserve the content of the block in the transpiled code, while also allowing us to reference it in a way that is unique and consistent.



anything to the contrary below is outdates and should be reshaped until otherwise valid

for the MLIR intermediary representation, we want to be able to generate MLIR code that is both correct and efficient and that preserves the semantics of the original code.  This is a key part of our approach to transpilation, and it allows us to handle complex C++ code that makes heavy use of templates and macros.  our graph should be precedent but our files should be tblgen and mlir based, with the graph as a source of truth for the relationships between constructs.  We want to be able to generate MLIR code that is both correct and efficient and that preserves the semantics of the original code.  This is a key part of our approach to transpilation, and it allows us to handle complex C++ code that makes heavy use of templates and macros.    
 
 no formatters.  we like terse code, 1-line conditionals, opt out of braces, and we prefer to use the ternary operator for simple conditionals.  We also prefer to use auto where possible, and we prefer to use range-based for loops.